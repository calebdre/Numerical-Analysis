\documentclass[11pt]{article}	% Everything after % in a line is comment

% Some commonly used packages. You can add more packages if you need
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{url}
\usepackage{multirow}

\title{An Analysis on Direct and Iterative Techniques to Solve Linear Systems}
\author{Ratislav Krylov \and Caleb Lewis}
% \date{} % Fill in actual date, or comment this line to show current date.

\newcommand\norm[1]{\left\lVert#1\right\rVert}


\begin{document}
\maketitle


\section{Introduction}
The need to solve the problem $Ax = b$ comes up in many real-world scenarios.
As a result, there have been many solutions of varying efficiency that
have been employed to solve it. In this paper, we look at 9 of these methods and
compare their complexity and accuracy.


\section{Methods}

\subsection{Gaussian Elimination}
Gaussian Elimination is one of the oldest methods used to solve systems of linear equations. It utilizes elementary row operations to convert the augmented system into an upper triangular matrix and then solve for each unknown $x$ through backwards substitution. In theory, Gaussian Elimination finds exact values for the system of linear equations, but due to physical limitations, its not really used in practice. There is always rounding in numerical computations and Gaussian Elimination is quite susceptible to them. Additionally its computational of $O(n^3)$ is quite high and makes impractical to use for large matrices. 

While this algorithm's complexity cannot be reduced, there exist techniques to deal with the effect that rounding has on Gaussian Elimination. 

    \subsubsection{Partial Pivoting}
    The round off errors accumulate the most when pivot is a lot smaller than other that it eliminates within its column. A simple way to deal with this is to find the largest value within the column before performing elimination and do a row swap if its greater than the current pivot. This way when eliminations are performed the rounding effect will not be as dramatic because the elimination values are smaller. 

    \subsubsection{Scaled Partial Pivoting}
    Partial Pivoting may not always work when the rows are not scaled properly to each other. The values of one row can be larger than those of another and the partial pivoting will not deal with rounding error properly. This can be easily fixed through dividing all the rows by their largest element. 
    
   	While Partial Pivoting and Scaling are nice and easy techniques to deal with rounding errors, they add more comparisons to an already computationally costly algorithm. But since the main reason why anyone would use Gaussian Elimination is to have as precise solutions as possible, these additional comparisons can be worth it.
   	
\subsection{Iterative Refinement}

Iterative Refinement is an algorithm used to take advantage of all computational digits available and to improve the accuracy of an acquired solution. Given $x_{approx}$ and $y$ the approximate solution to the system $A\textbf{y} = \textbf{r}$ where r is the residual that can be calculated through $x_{approx}$, we can use the fact that $y \approx x - x_{approx}$ and calculate a better estimate $x_{approx} + y$ for $x$. It is often used together with Gaussian Elimination since it can theoretically calculate the exact solution if not for rounding errors. Combined with Iterative Refinement it actually allows us to compute the exact answer for the system of linear equations upto the rounding digits. 


\subsection{Jacobi Iterative}
    The first of the iterative methods that we consider is the Jacobi Iterative method. For a diagnolly dominant matrix $A$,
    \begin{equation}\label{eq:jacobi-eq-1-qualifier}
        \textrm{let} \quad x^{(0)}\in {\mathbb R}^n,\quad D = diag(A),\quad R = A - D
    \end{equation}

    \begin{equation}\label{eq:jacobi-eq-1}
        x^{k+1} = D^{-1}(b- Rx^{(k)})
    \end{equation}
    We repeat the above equation for $k = 0, 1, ...$ until convergence. The stopping critia is:

    \begin{equation}
        \frac{\norm{x^{(k)} - x^{(k-1)}}}{x^{(k)}} \leq \epsilon
    \end{equation}
    For our experiments, we set $\epsilon = .001$ as we found it was enough to show the differences in accuracies and behaviors between this and other methods. The following is a table of the results after running the method on the examples for a various number of iterations:
    \begin{center}
        \begin{tabular}{||c|c|c|c|c||}
            \hline
            \multirow{2}{5em}{Example 2} & 10 & 20 & 30 & 40 \\ [.25em]
            & 0.52058755 & 0.17910029 & 0.20600456 & 0.02831351 \\ [.25em]
            \hline \hline
            \multirow{2}{5em}{Example 3} & 10 & 20 & 30 & 40 \\ [.25em]
            & 0.46132609 & 0.04399548 & .00419573634 & .000400136598 \\ [.25em]
            \hline
        \end{tabular}
    \end{center}

    Notice that examples 1 and 4 have been left out - those are not diagonally dominant, therefore the Jacobi method
    is not able to converge.

\subsection{Gauss-Seidel}
Next is the Gauss-Seidel method. This method has similar limitations as the Jacobi method, but performs better in practice.
First we define $L$ to be the lower triangle part of $A$ and:

\begin{equation}\label{eq:jacobi-eq-1}
    let x^{(0)}\in {\mathbb R}^n, \quad U = A - L
\end{equation}
the method is the iteration
\begin{equation}\label{eq:jacobi-eq-1}
    x^{k+1} = L^{-1}(b - Ux^{(k)})
\end{equation}

for $k = 0, 1, ...$ until the convergence or stopping criteria. We will use the same criteria as we did in the Jacobi method.

\begin{center}
    \begin{tabular}{||c|c|c|c|c||}
        \hline
        \multirow{2}{5em}{Example 2} & 10 & 20 & 30 & 40 \\ [.25em]
        & 0.1327548 & .00326896 & .00292537 & .003067423 \\ [.25em]
        \hline \hline
        \multirow{2}{5em}{Example 3} & 10 & 20 & 30 & 40 \\ [.25em]
        & 0.02302011 & .00033498 & .00000190 & .00000002 \\ [.25em]
        \hline
    \end{tabular}
\end{center}
Examples 1 and 4 have been left out due to the same reasons as the Jacobi method - this method does not converge on matrices that are not diagnolly dominant. Also notice that on example 2, the method does not get more accurate after iteration 20 - it seems to only oscilate without improving.

\subsection{Successive Over-Relaxation}
stuff abuot Successive Over-Relaxation

\subsection{(Preconditioned) Conjugate Gradient Method}
stuff abuot (Preconditioned) Conjugate Gradient Method

\section{Numerical experiments}
\subsection{Example 1}

\subsection{Example 2}

\subsection{Example 3}

\subsection{Example 4}

\section{Discussion}
This is a major part for this project. It should constitute your findings and thoughts.
Based on the tests you have,
you want to comment on the performance of these methods and how would you
suggest to use in practice. Have extensive discussions with your team members
and give detailed reasonings for your claims. You can cite books, papers, or other



\end{document}
