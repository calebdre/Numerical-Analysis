\documentclass[11pt]{article}	% Everything after % in a line is comment

% Some commonly used packages. You can add more packages if you need
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{url}
\usepackage{multirow}

\title{An Analysis on Direct and Iterative Techniques to Solve Linear Systems}
\author{Ratislav Krylov \and Caleb Lewis}
% \date{} % Fill in actual date, or comment this line to show current date.

\newcommand\norm[1]{\left\lVert#1\right\rVert}


\begin{document}
\maketitle

\begin{abstract}
    The need to solve the problem $Ax = b$ comes up in many real-world scenarios.
    As a result, there have been many solutions of varying efficiencies that people
    have employed to solve it. In this paper, we look at 9 of these methods and
    compare their complexity and accuracy.
\end{abstract}

\section{Introduction}
A section briefly introduces the background and purpose of this paper.

\section{Examples}
In our analysis of each method, we will be using 4 sample matrices. These samples represent different conditions that will affect how well each method will perform.

\begin{math}
    \begin{alignedat}{3}
        A = \begin{bmatrix}
                1 & 1 & 0 & 3 \\
                2 & 1 &  -1 & 1 \\
                3 & -1 & -1 & 2 \\
                -1 & 2 & 3 & -1  \\
            \end{bmatrix}
        ,&& \quad
        b = \begin{bmatrix}
                4 & 1 & -3 & 4
            \end{bmatrix}
        ,&& \quad
        x = \begin{bmatrix}
                -1 & 2 & 0 & 1
            \end{bmatrix}\\
        A = \begin{bmatrix}
                .2 & .1 & 1 & 1 & 0 \\
                .1 & 4 & -1 & 1 & -1 \\
                1 & -1 & 60 & 0 & -1 \\
                1 & 1 & 0 & 8 & 4  \\
                0 & -1 & -2 & 4 & 700
            \end{bmatrix}
        ,&& \quad
        b = \begin{bmatrix}
                1 & 2 & 3 & 4 & 5
            \end{bmatrix}
        ,&&
        x^T = \begin{bmatrix}
                7.8597130   \\
                0.4229264  \\
                -.0735922 \\
                -.5406430  \\
                01062616286
            \end{bmatrix}\\
        A = \begin{bmatrix}
                4 & 3 & 0 \\
                3 & 4 & -1 \\
                0 & -1 & 4 \\
            \end{bmatrix}
        ,&&
        b = \begin{bmatrix}
                24 & 30 & -24
            \end{bmatrix}
        ,&& \quad
        x = \begin{bmatrix}
                3 & 4 & -5
            \end{bmatrix}\\
        A = \begin{bmatrix}
                3.3330 & 15920 & -10.333\\
                2.2220 & 16.710 & 9.6120\\
                1.5611 & 5.1791 & 1.6852\\
            \end{bmatrix}
        ,&& \quad
        b = \begin{bmatrix}
                15913 & 28.544 & 8.4254\\
            \end{bmatrix}
        ,&& \quad
        x = \begin{bmatrix}
                1 & 1 & 1
            \end{bmatrix}
    \end{alignedat}
\end{math}

\section{Methods}

\subsection{Gauss Elimination}

    \subsubsection{No Pivoting}
    stuff about no Pivoting

    \subsubsection{Partial Pivoting}
    stuff abuot partial Pivoting

    \subsubsection{Scaled Partial Pivoting}
    stuff abuot scaled partial Pivoting


\subsection{Jacobi Iterative}
    The first of the iterative methods that we consider is the Jacobi Iterative method. For a diagnolly dominant matrix $A$,
    \begin{equation}\label{eq:jacobi-eq-1-qualifier}
        \textrm{let} \quad x^{(0)}\in {\mathbb R}^n,\quad D = diag(A),\quad R = A - D
    \end{equation}

    \begin{equation}\label{eq:jacobi-eq-1}
        x^{k+1} = D^{-1}(b- Rx^{(k)})
    \end{equation}
    We repeat the above equation for $k = 0, 1, ...$ until convergence. The stopping critia is:

    \begin{equation}
        \frac{\norm{x^{(k)} - x^{(k-1)}}}{x^{(k)}} \leq \epsilon
    \end{equation}
    For our experiments, we set $\epsilon = .001$ as we found it was enough to show the differences in accuracies and behaviors between this and other methods. The following is a table of the results after running the method on the examples for a various number of iterations:
    \begin{center}
        \begin{tabular}{||c|c|c|c|c||}
            \hline
            \multirow{2}{5em}{Example 2} & 10 & 20 & 30 & 40 \\ [.25em]
            & 0.52058755 & 0.17910029 & 0.20600456 & 0.02831351 \\ [.25em]
            \hline \hline
            \multirow{2}{5em}{Example 3} & 10 & 20 & 30 & 40 \\ [.25em]
            & 0.46132609 & 0.04399548 & .00419573634 & .000400136598 \\ [.25em]
            \hline
        \end{tabular}
    \end{center}

    Notice that examples 1 and 4 have been left out - those are not diagnolly dominant, therefore the Jacobi method
    is not able to converge.

\subsection{Gauss-Seidel}
Next is the Gauss-Seidel method. This method has similar limitations as the Jacobi method, but performs better in practice.
First we define $L$ to be the lower triangle part of $A$ and:

\begin{equation}\label{eq:gauss-seidel-eq-1}
    x^{(0)}\in {\mathbb R}^n, \quad U = A - L
\end{equation}
the method is the iteration
\begin{equation}\label{eq:gauss-seidel-eq-2}
    x^{k+1} = L^{-1}(b - Ux^{(k)})
\end{equation}

for $k = 0, 1, ...$ until the convergence or stopping criteria. We will use the same criteria as we did in the Jacobi method.

\begin{center}
    \begin{tabular}{||c|c|c|c|c||}
        \hline
        \multirow{2}{5em}{Example 2} & 10 & 17 & 20 & 30 \\ [.25em]
        & .1327548 & .01452508 & .00144870 & .00292537 \\ [.25em]
        \hline \hline
        \multirow{2}{5em}{Example 3} & 10 & 13 & 20 & 30 \\ [.25em]
        & .02302011 & .00899223 & .00033498 & .00000190 \\ [.25em]
        \hline
    \end{tabular}
\end{center}
Examples 1 and 4 have been left out due to the same reasons as the Jacobi method - this method does not converge on matrices that are not diagnolly dominant. The method converges at 17 and 13 iterations for examples 2 and 3 respectively.

\subsection{Successive Over-Relaxation}
The Successive Over-Relaxation (SOR) technique is an improvement on the Gauss-Seidel method by introducing a \textbf{relaxation} parameter to over-correct for the error at each step. The iterative method is given by:

\begin{equation}\label{eq:successive-over-relax-eq-1}
    x^{k+1} = (D - \omega L)^{-1}[(1 - \omega)D + \omega U]x^{(k-2)} + \omega(D - \omega L){-1}b
\end{equation}
where $D, -L, \textrm{and} -U$ are the diagnol, strict lower, and string upper triangular parts of A respectively. There is an optimal value of $0 < \omega < 2$ where this method converges the fastest. SOR is only stable where $\omega > 2$ and is under-relaxed where $0 < \omega < 1$.

\begin{center}
    \begin{tabular}{||c|c|c|c|c||}
        \hline
        \multirow{2}{5em}{Example 2} & 10 & 14 &  20 & 30 \\ [.25em]
        & .04903008 & .00560935 & .00294247 & .00307403485 \\ [.25em]
        \hline\hline
        \multirow{2}{5em}{Example 3} & 8 & 10 & 20 & 30 \\ [.25em]
        & .00101802 & .00005610 & .000000005 & .00000000 \\ [.25em]
        \hline
    \end{tabular}
\end{center}

Note that SOR converges at 14 \& 8 iterations for examples 2 and 3 respectively. It does indeed converge faster than the Gauss Seidel method as it converges at 17 and 13 iterations and the errors between each iteration are quite small.

\subsection{Iterative Refinement}
stuff abuot Iterative Refinement

\subsection{(Preconditioned) Conjugate Gradient Method (PCG)}
The Preconditioned Conjugate Gradient (PCG) Method is the last of the methods we will explore. Given a preconditioner $C, x^{(0)}, r^{(0)} = b - Ax^{(0)}, w^{(0)} = C^{-1}r^{(0)}, v^{(1)} = C^{-T}w^{(0)}$, it is described by the following:
\begin{align*}\label{eq:precondition-conj-grad-eq-1}
    t_k = \frac{\langle w^{(k-1)}, w^{(k-1)}\rangle}{\langle v^{(k)}, Av^{(k)}\rangle} \\\\
    x^{(k)} = x^{(k-1)} + t_kv^{(k)} \\\\
    r^{(k)} = r^{(k-1)} - t_kAv^{(k)} \\\\
    w^{(k)} = C^{-1}r^{(k)} \\\\
    s_k = \frac{\langle w^{(k)}, w^{(k)}\rangle}{\langle w^{(k-1)}, w^{(k-1)}\rangle} \\\\
    v^{(k+1)} = C^{-T}w^{(k)} + s_kv^{(k)}
\end{align*}
PCG is a modified version of gradient descent that corrects for the direction and magnitude problem by using the fact that the residual of the matrix is also the steepest descent direction and its orthogonality with off-diagnol vectors. With this, we can solve for scalars $t_k, s_k$ to maximize direction and magnitude for each iteration. This is further optimized by preconditioner C.

\begin{center}
    \begin{tabular}{||c|c|c||}
        \hline
        \multirow{2}{5em}{Example 2} & 10 & 21 \\ [.25em]
        & 0303791568] & .00298286092\\ [.25em]
        \hline\hline
        \multirow{2}{5em}{Example 3} & 4 & 10 \\ [.25em]
        & .000000000 & .000000000 \\ [.25em]
        \hline
        \multirow{2}{5em}{Example 4} & 1 & 10 \\ [.25em]
        & .0 & .0 \\ [.25em]
        \hline
    \end{tabular}
\end{center}


\section{Numerical experiments}
You will need to demonstrate the performance
of the methods on several (ideally 3 to 5) example IVPs (of your own choice).
You can choose some problems from textbook, but make sure that you
explicitly state what the problem you chose for each test.

To show the performance, it is often better to use figures
rather than tables (unless there are very few numbers to show).
For example, you can show the result of RK4
using Figure If you have multiple results, you can plot each
with a curve (in different color/line-style/marker type) in the plot. If they are too close, you can consider to
plot $|w_i-y_i|$, the error of estimate $w_i$ to true solution $y_i=y(t_i)$,
instead of actual $y_i$ and $w_i$. This way, you can see which methods have lower errors (higher accuracy).

\section{Discussion}
This is a major part for this project. It should constitute your findings and thoughts.
Based on the tests you have,
you want to comment on the performance of these methods and how would you
suggest to use in practice. Have extensive discussions with your team members
and give detailed reasonings for your claims. You can cite books, papers, or other


\section{Summary}
A quick summary to conclude the term paper using a paragraph or two.

\end{document}
